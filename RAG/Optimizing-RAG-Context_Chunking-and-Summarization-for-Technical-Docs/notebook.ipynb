{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing RAG Context: Chunking and Summarization for Technical Docs\n",
    "\n",
    "This notebook contains the code implementation for my blog post on [dev.to](https://dev.to/oleh-halytskyi/optimizing-rag-context-chunking-and-summarization-for-technical-docs-3pel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Overview\n",
    "\n",
    "![Solution Overview](images/solution_overview.png)\n",
    "\n",
    "- Loading and Converting Documents\n",
    "- Custom Text Splitter\n",
    "- Summarization with Header Hierarchy\n",
    "- VectorDB Integration\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Environment\n",
    "\n",
    "Begin by setting up a [Conda](https://conda.io/projects/conda/en/latest/user-guide/install/macos.html) environment to manage dependencies and keep the project isolated.\n",
    "\n",
    "```bash\n",
    "# Create a new environment called 'rag-env'\n",
    "conda create -n rag-env python=3.11\n",
    "\n",
    "# Activate the environment\n",
    "conda activate rag-env\n",
    "\n",
    "# Install necessary packages\n",
    "pip install langchain==0.2.16 \\\n",
    "    langchain_community==0.2.16 \\\n",
    "    beautifulsoup4==4.12.3 \\\n",
    "    markdownify==0.13.1 \\\n",
    "    tiktoken==0.7.0 \\\n",
    "    langchain-chroma==0.1.3\n",
    "```\n",
    "\n",
    "Additionally, you’ll need to install [Ollama](https://ollama.com) locally if you haven’t already. This is required for running the [llama3.1](https://ollama.com/library/llama3.1) and [mxbai-embed-large](https://ollama.com/library/mxbai-embed-large) models for summarization and embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents\n",
    "\n",
    "[LangChain's RecursiveUrlLoader](https://python.langchain.com/v0.2/docs/integrations/document_loaders/recursive_url) is used to load HTML documents. This loader extracts content from web pages, and in this example, the _Python Control Flow_ tutorial is loaded with a depth limit of 1 to avoid following additional links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "\n",
    "# Load the document\n",
    "loader = RecursiveUrlLoader(\n",
    "    \"https://docs.python.org/3/tutorial/controlflow.html\",\n",
    "    max_depth=1,\n",
    ")\n",
    "\n",
    "html_docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert HTML Document to Markdown Format\n",
    "\n",
    "In this step, a custom transformer based on [MarkdownifyTransformer](https://python.langchain.com/v0.2/docs/integrations/document_transformers/markdownify) is used. This custom transformer overrides the `transform_documents` method to process content and ensure that code blocks are handled properly. A regular expression is applied to identify code blocks and remove unnecessary newlines before the closing backticks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_transformers import MarkdownifyTransformer\n",
    "import re\n",
    "\n",
    "# Custom transformer that extends the MarkdownifyTransformer to strip trailing newlines from code blocks\n",
    "class CustomMarkdownifyTransformer(MarkdownifyTransformer):\n",
    "    def transform_documents(self, documents):\n",
    "        transformed_docs = super().transform_documents(documents)\n",
    "        for doc in transformed_docs:\n",
    "            if hasattr(doc, 'page_content'):\n",
    "                doc.page_content = self._strip_trailing_newline_from_code_blocks(doc.page_content)\n",
    "        return transformed_docs\n",
    "\n",
    "    def _strip_trailing_newline_from_code_blocks(self, text):\n",
    "        # Regex to find code blocks and ensure they end with a newline before the closing backticks\n",
    "        code_block_pattern = re.compile(r'(```\\w*\\n[\\s\\S]*?)```')\n",
    "        return code_block_pattern.sub(lambda match: match.group(1).rstrip() + '\\n```', text)\n",
    "\n",
    "# Transform the document to Markdown format\n",
    "md = CustomMarkdownifyTransformer()\n",
    "md_docs = md.transform_documents(html_docs)\n",
    "\n",
    "# Save the Markdown document\n",
    "with open('files/generated/md_docs.md', 'w') as f:\n",
    "    f.write(md_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated document is available in the [files/generated/md_docs.md](./files/generated/md_docs.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Markdown into Chunks Based on Headers\n",
    "\n",
    "To prepare the document for querying in a RAG application, the Markdown content is split into chunks based on its structure, using document headers as delimiters.\n",
    "\n",
    "During the development of this custom splitter, existing approaches like [LangChain's Text Splitters](https://python.langchain.com/v0.2/docs/how_to/#text-splitters) and the [ExperimentalMarkdownSyntaxTextSplitter](https://api.python.langchain.com/en/latest/markdown/langchain_text_splitters.markdown.ExperimentalMarkdownSyntaxTextSplitter.html) were tested. However, they often distorted code blocks or introduced unwanted whitespace, making them unsuitable for the task.\n",
    "\n",
    "To overcome these challenges, a custom splitter was developed that:\n",
    "\n",
    "- Preserves the logical flow by splitting only on headers;\n",
    "- Ensures code blocks remain intact without altering formatting;\n",
    "- Filters out irrelevant headers like \"Table of Contents\", \"This Page\", and \"Navigation\";\n",
    "- Adds hierarchical header metadata for each chunk, maintaining context and aiding in summarization and flexible querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks by headers: 23\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "sicbh_include_headers_in_content = False\n",
    "sicbh_filter_headers = [\"Table of Contents\", \"This Page\", \"Navigation\"]\n",
    "sicbh_show_unwanted_chunks_metadata = False\n",
    "\n",
    "# Function to divide the Markdown documents into chunks based on headers\n",
    "def split_into_chunks_by_headers(md_docs):\n",
    "    chunks = []\n",
    "    header_pattern = re.compile(r'^(#{1,6})\\s+(.*)')\n",
    "    code_block_pattern = re.compile(r'^\\s*```')\n",
    "    in_code_block = False\n",
    "\n",
    "    for doc in md_docs:\n",
    "        if hasattr(doc, 'page_content'):\n",
    "            lines = doc.page_content.split('\\n')\n",
    "        else:\n",
    "            raise AttributeError(\"Document object has no 'page_content' attribute\")\n",
    "\n",
    "        current_chunk = {'metadata': {}, 'content': ''}\n",
    "        current_headers = {}\n",
    "        prev_header_level = 0\n",
    "\n",
    "        for line in lines:\n",
    "            if code_block_pattern.match(line):\n",
    "                in_code_block = not in_code_block\n",
    "\n",
    "            if not in_code_block:\n",
    "                match = header_pattern.match(line)\n",
    "                if match:\n",
    "                    # If there is content in the current chunk, add it to the chunks list\n",
    "                    if current_chunk['content']:\n",
    "                        current_chunk['content'] = current_chunk['content'].strip()\n",
    "                        chunks.append(current_chunk)\n",
    "                        current_chunk = {'metadata': {}, 'content': ''}\n",
    "\n",
    "                    # Extract the header level and text\n",
    "                    header_level = len(match.group(1))\n",
    "                    header_text = match.group(2)\n",
    "\n",
    "                    # Clean the header text\n",
    "                    header_text = re.sub(r'\\\\', '', header_text)\n",
    "                    header_text = re.sub(r'\\[¶\\]\\(.*?\\)', '', header_text).strip()\n",
    "\n",
    "                    # Update the current headers\n",
    "                    header_key = f'Header {header_level}'\n",
    "                    if header_level > prev_header_level:\n",
    "                        current_headers[header_key] = header_text\n",
    "                    else:\n",
    "                        del current_headers[f'Header {prev_header_level}']\n",
    "                        current_headers[header_key] = header_text\n",
    "\n",
    "                    # Add the header line to metadata\n",
    "                    current_chunk['metadata'] = current_headers.copy()\n",
    "\n",
    "                    # Optionally add the cleaned header text to content\n",
    "                    if sicbh_include_headers_in_content:\n",
    "                        current_chunk['content'] += f\"{match.group(1)} {header_text}\\n\"\n",
    "\n",
    "                    # Update the previous header level\n",
    "                    prev_header_level = header_level\n",
    "                else:\n",
    "                    current_chunk['content'] += line + '\\n'\n",
    "            else:\n",
    "                current_chunk['content'] += line + '\\n'\n",
    "\n",
    "        # Add the last chunk to the chunks list\n",
    "        if current_chunk['content']:\n",
    "            current_chunk['content'] = current_chunk['content'].strip()\n",
    "            chunks.append(current_chunk)\n",
    "\n",
    "    # Convert the chunks to Document objects, filtering out unwanted chunks\n",
    "    documents = []\n",
    "    unwanted_chunks = []\n",
    "    for chunk in chunks:\n",
    "        metadata = chunk['metadata']\n",
    "        if metadata and not any(any(unwanted in value for unwanted in sicbh_filter_headers) for value in metadata.values()):\n",
    "            documents.append(Document(page_content=chunk['content'], metadata=chunk['metadata']))\n",
    "        else:\n",
    "            unwanted_chunks.append(chunk['metadata'])\n",
    "\n",
    "    # Optionally print the unwanted chunks metadata\n",
    "    if sicbh_show_unwanted_chunks_metadata and unwanted_chunks:\n",
    "        print(f\"Unwanted chunks metadata:\")\n",
    "        for chunk in unwanted_chunks:\n",
    "            print(chunk)\n",
    "        print()\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Split the Markdown documents into chunks based on headers\n",
    "chunks_by_headers = split_into_chunks_by_headers(md_docs)\n",
    "print(f\"Number of chunks by headers: {len(chunks_by_headers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Chunks into Smaller Chunks Based on Tokens\n",
    "\n",
    "At this point, the document has already been split into chunks based on headers. For RAG applications, it's necessary to ensure that each chunk fits within the token limit supported by the language model. While this step can be skipped when using models with large context lengths, splitting large chunks is still useful for optimal performance.\n",
    "\n",
    "This custom token-based splitter:\n",
    "\n",
    "- Splits the document into smaller chunks while preserving sentences and code blocks;\n",
    "- Avoids splitting sentences that end with \":\" from the following code or text;\n",
    "- Optionally prevents splitting text directly after a code block, as it often contains explanations.\n",
    "\n",
    "The goal is to preserve the full meaning of the text in each chunk, rather than strictly focusing on token accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks by tokens: 33\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tiktoken_encoder = \"cl100k_base\"\n",
    "chunk_max_tokens = 500\n",
    "scbt_text_follow_code_block = True\n",
    "\n",
    "# Function to split a chunk into smaller parts based on token count\n",
    "def split_chunk_by_tokens(content, tokenizer, max_tokens):\n",
    "    # Split content into code blocks and paragraphs\n",
    "    parts = re.split(r'(\\n```\\n.*?\\n```\\n)', content, flags=re.DOTALL)\n",
    "    final_parts = []\n",
    "    for part in parts:\n",
    "        if part.startswith('\\n```\\n') and part.endswith('\\n```\\n'):\n",
    "            final_parts.append(part)\n",
    "        else:\n",
    "            final_parts.extend(re.split(r'\\n\\s*\\n', part))\n",
    "    # Remove newlines from the start and end of each part\n",
    "    parts = [part.strip() for part in final_parts if part.strip()]\n",
    "\n",
    "    # Calculate total tokens\n",
    "    total_tokens = sum(len(tokenizer.encode(part)) for part in parts)\n",
    "    target_tokens_per_chunk = total_tokens // (total_tokens // max_tokens + 1)\n",
    "\n",
    "    # Initialize variables\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_token_count = 0\n",
    "\n",
    "    # Iterate over the parts and merge them if needed\n",
    "    i = 0\n",
    "    while i < len(parts):\n",
    "        part = parts[i]\n",
    "\n",
    "        # Merge parts if the current part ends with \":\" or \"```\" (if enabled) and has a following part\n",
    "        while (part.endswith(\":\") or (scbt_text_follow_code_block and part.endswith(\"```\"))) and i + 1 < len(parts):\n",
    "            part += \"\\n\\n\" + parts[i + 1]\n",
    "            i += 1  # Skip the next part as it has been merged\n",
    "\n",
    "        # Calculate the token count of the part\n",
    "        part_tokens = tokenizer.encode(part)\n",
    "        part_token_count = len(part_tokens)\n",
    "\n",
    "        # Split the part into smaller parts if it exceeds the target token count\n",
    "        if current_token_count + part_token_count > target_tokens_per_chunk and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = part\n",
    "            current_token_count = part_token_count\n",
    "        else:\n",
    "            current_chunk += \"\\n\\n\" + part if current_chunk else part\n",
    "            current_token_count += part_token_count\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Add the last chunk if it has content\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Function to divide the Markdown documents into chunks based on token count\n",
    "def split_into_chunks_by_tokens(chunks, tokenizer, max_tokens):\n",
    "    split_chunks = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        token_count = len(tokenizer.encode(chunk.page_content))\n",
    "        if token_count > max_tokens:\n",
    "            split_texts = split_chunk_by_tokens(chunk.page_content, tokenizer, max_tokens)\n",
    "            for text in split_texts:\n",
    "                split_chunks.append(Document(page_content=text, metadata=chunk.metadata))\n",
    "        else:\n",
    "            split_chunks.append(chunk)\n",
    "\n",
    "    return split_chunks\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = tiktoken.get_encoding(tiktoken_encoder)\n",
    "\n",
    "# Split the chunks into smaller parts based on token count\n",
    "chunked_docs = split_into_chunks_by_tokens(chunks_by_headers, tokenizer, chunk_max_tokens)\n",
    "print(f\"Number of chunks by tokens: {len(chunked_docs)}\")\n",
    "\n",
    "# Save the chunked documents to a file\n",
    "chunked_docs_file_content = \"\"\n",
    "for doc in chunked_docs:\n",
    "    token_count = len(tokenizer.encode(doc.page_content))\n",
    "    chunked_docs_file_content += f\"\\nToken count: {token_count}\\n\"\n",
    "    chunked_docs_file_content += f\"Metadata: {doc.metadata}\\n\"\n",
    "    chunked_docs_file_content += f\"Content:\\n{doc.page_content}\\n\\n\"\n",
    "\n",
    "with open('files/generated/chunked_docs.txt', 'w') as f:\n",
    "    f.write(chunked_docs_file_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated chunked documents is available in the [files/generated/chunked_docs.txt](./files/generated/chunked_docs.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization Based on Headers and Chunk Text\n",
    "\n",
    "The next step is to generate summaries for each chunk. The [llama3.1](https://ollama.com/library/llama3.1) model via `ChatOllama` is used to create concise summaries that incorporate both the content of the chunk and all relevant headers in the hierarchy. This ensures that the summary retains the full context of the document, including its structure.\n",
    "\n",
    "By considering the hierarchical headers, the summaries maintain the logical flow of the document, which is particularly useful when querying from VectorDB. This helps ensure accurate and relevant information retrieval by preserving the overall meaning and structure of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of summarized documents: 33\n",
      "Number of original documents: 33\n",
      "Number of all documents: 66\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "import uuid\n",
    "\n",
    "# Initialize the ChatOllama model\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "# Create the prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"Summarize the following content in a single, concise paragraph. \"\n",
    "                \"Include key information from all headers provided, maintaining the overall context and meaning. \"\n",
    "                \"Output only the summary text without any introductory phrases, labels, or concluding remarks.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{context}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Summarize the content of the documents\n",
    "summarized_docs = []\n",
    "for doc in chunked_docs:\n",
    "    # Merge the metadata and content into a single document\n",
    "    metadata = \"\\n\".join(f\"{key}: {value}\" for key, value in doc.metadata.items())\n",
    "    merged_docs = [Document(page_content=metadata), Document(page_content=doc.page_content)]\n",
    "\n",
    "    # Invoke the chain to summarize the content\n",
    "    result = chain.invoke({\"context\": merged_docs})\n",
    "\n",
    "    # Create a new Document object with the summary content\n",
    "    unique_id = str(uuid.uuid4())\n",
    "    summary_doc = Document(page_content=result, metadata={\"type\": \"summary\", \"id\": unique_id})\n",
    "\n",
    "    # Create a copy of the metadata and update it\n",
    "    updated_metadata = doc.metadata.copy()\n",
    "    updated_metadata.update({\"type\": \"original\", \"summary_id\": unique_id})\n",
    "    doc.metadata = updated_metadata\n",
    "    summarized_docs.append(summary_doc)\n",
    "\n",
    "# Merge the summarized and original documents\n",
    "all_docs = summarized_docs + chunked_docs\n",
    "\n",
    "print(f\"Number of summarized documents: {len(summarized_docs)}\")\n",
    "print(f\"Number of original documents: {len(chunked_docs)}\")\n",
    "print(f\"Number of all documents: {len(all_docs)}\")\n",
    "\n",
    "# Save the summarized documents to a file\n",
    "summarized_docs_file_content = \"\"\n",
    "for doc in summarized_docs:\n",
    "    summarized_docs_file_content += f\"Metadata: {doc.metadata}\\n\"\n",
    "    summarized_docs_file_content += f\"Content: {doc.page_content}\\n\\n\"\n",
    "\n",
    "with open('files/generated/summarized_docs.txt', 'w') as f:\n",
    "    f.write(summarized_docs_file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated summarized documents is available in the [files/generated/summarized_docs.txt](./files/generated/summarized_docs.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Documents to VectorDB\n",
    "\n",
    "After summarizing the documents, the next step is to store them in a vector database to enable efficient querying and retrieval based on both summaries and original content. The [mxbai-embed-large](https://ollama.com/library/mxbai-embed-large) model from [Ollama](https://ollama.com) is used to generate embeddings for both the original and summarized documents.\n",
    "\n",
    "The embeddings capture the semantic meaning of the documents, making it easier for the VectorDB to retrieve relevant chunks during queries. In this example, the documents are stored in Chroma, a vector database optimized for efficient querying and retrieval.\n",
    "\n",
    "The key steps include:\n",
    "\n",
    "- Initializing `OllamaEmbeddings` with the [mxbai-embed-large](https://ollama.com/library/mxbai-embed-large) model;\n",
    "- Clearing any existing documents in the Chroma VectorDB;\n",
    "- Adding the summarized and original documents to the VectorDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Initialize the Ollama embedding model\n",
    "ollama_emb = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large\",\n",
    ")\n",
    "\n",
    "# Initialize Chroma vector store and clear existing documents\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"summarization\",\n",
    ")\n",
    "vectorstore.delete_collection()\n",
    "\n",
    "# Add new documents to the collection\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_docs,\n",
    "    embedding=ollama_emb,\n",
    "    collection_name=\"summarization\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Documents from VectorDB\n",
    "\n",
    "The final step is to query the documents stored in the Chroma vector database and retrieve the relevant results. Before running the queries, the retriever is prepared, and a function is set up to output the results in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the vector store as a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Function to print results\n",
    "def print_results(title, results):\n",
    "    print(title)\n",
    "    print()\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"---------- Result #{i + 1} ----------\")\n",
    "        print(f\"Metadata: {result.metadata}\")\n",
    "        print(f\"Content: {result.page_content}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the retriever prepared and the `print_results` function ready, various queries can now be performed to demonstrate the flexibility of querying both summaries and original content in the vector database:\n",
    "\n",
    "- Querying summaries only;\n",
    "- Querying original content;\n",
    "- Querying both summaries and original content together;\n",
    "- Querying summaries but retrieving the original text based on those summaries.\n",
    "\n",
    "By applying filters for summary and original content, the vector database can be queried flexibly to retrieve the most relevant results based on specific needs.\n",
    "\n",
    "### Querying Summaries\n",
    "\n",
    "In this example, only the summaries are queried, allowing retrieval of concise, context-rich overviews of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Results.\n",
      "\n",
      "---------- Result #1 ----------\n",
      "Metadata: {'id': 'd0f21c29-decf-405b-828f-8b714eebd459', 'type': 'summary'}\n",
      "Content: The `range()` function returns an object that behaves like a list but doesn't actually make one, saving space. This iterable object can be used with functions and constructs that expect successive items until the supply is exhausted, such as the `for` statement or the `sum()` function. When printed directly, it displays its start and end values, e.g., `range(0, 10)`.\n",
      "\n",
      "\n",
      "---------- Result #2 ----------\n",
      "Metadata: {'id': '1ff36254-aa51-4302-b0f1-aeba92212a96', 'type': 'summary'}\n",
      "Content: The built-in `range()` function generates arithmetic progressions that can be used for iteration over a sequence of numbers. It takes three parameters: start point, end point, and step (default is 1), and returns an iterator that produces the specified range of values. For example, `range(5)` generates numbers from 0 to 4, while `range(5, 10)` generates numbers from 5 to 9. The `range()` function can also be used in combination with `len()` to iterate over the indices of a sequence, or with other functions like `enumerate()` for more convenient looping techniques.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query only summaries\n",
    "retriever.search_kwargs = {\"k\": 2, \"filter\": {\"type\": \"summary\"}}\n",
    "summary_results = retriever.invoke(\"I want to write a Python script that prints numbers from 1 to 30.\")\n",
    "print_results(\"Summary Results.\", summary_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output demonstrates two concise summaries related to a query about writing a Python script that prints numbers from 1 to 30. Both summaries provide an overview of Python's `range()` function, explaining its use in iterating over sequences of numbers. The summaries are context-rich, mentioning how the `range()` function works in a loop and how it can be utilized with additional constructs like `sum()` and `enumerate()`.\n",
    "\n",
    "This is a good match for the query, as the `range()` function is a common tool for iterating over a sequence of numbers, directly relating to the task of printing numbers from 1 to 30.\n",
    "\n",
    "### Querying Original Content\n",
    "\n",
    "Next, only the original content is queried to retrieve more detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Content Results.\n",
      "\n",
      "---------- Result #1 ----------\n",
      "Metadata: {'Header 1': '4. More Control Flow Tools', 'Header 2': '4.3. The [`range()`](../library/stdtypes.html#range \"range\") Function', 'summary_id': '1ff36254-aa51-4302-b0f1-aeba92212a96', 'type': 'original'}\n",
      "Content: If you do need to iterate over a sequence of numbers, the built\\-in function\n",
      "[`range()`](../library/stdtypes.html#range \"range\") comes in handy. It generates arithmetic progressions:\n",
      "\n",
      "```\n",
      ">>> for i in range(5):\n",
      "...     print(i)\n",
      "...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "```\n",
      "\n",
      "The given end point is never part of the generated sequence; `range(10)` generates\n",
      "10 values, the legal indices for items of a sequence of length 10\\. It\n",
      "is possible to let the range start at another number, or to specify a different\n",
      "increment (even negative; sometimes this is called the ‘step’):\n",
      "\n",
      "```\n",
      ">>> list(range(5, 10))\n",
      "[5, 6, 7, 8, 9]\n",
      "\n",
      ">>> list(range(0, 10, 3))\n",
      "[0, 3, 6, 9]\n",
      "\n",
      ">>> list(range(-10, -100, -30))\n",
      "[-10, -40, -70]\n",
      "```\n",
      "\n",
      "To iterate over the indices of a sequence, you can combine [`range()`](../library/stdtypes.html#range \"range\") and\n",
      "[`len()`](../library/functions.html#len \"len\") as follows:\n",
      "\n",
      "```\n",
      ">>> a = ['Mary', 'had', 'a', 'little', 'lamb']\n",
      ">>> for i in range(len(a)):\n",
      "...     print(i, a[i])\n",
      "...\n",
      "0 Mary\n",
      "1 had\n",
      "2 a\n",
      "3 little\n",
      "4 lamb\n",
      "```\n",
      "\n",
      "In most such cases, however, it is convenient to use the [`enumerate()`](../library/functions.html#enumerate \"enumerate\")\n",
      "function, see [Looping Techniques](datastructures.html#tut-loopidioms).\n",
      "\n",
      "\n",
      "---------- Result #2 ----------\n",
      "Metadata: {'Header 1': '4. More Control Flow Tools', 'Header 2': '4.7. Defining Functions', 'summary_id': '0a569fde-d1df-4f7c-9315-1e6da9b15214', 'type': 'original'}\n",
      "Content: We can create a function that writes the Fibonacci series to an arbitrary\n",
      "boundary:\n",
      "\n",
      "```\n",
      ">>> def fib(n):    # write Fibonacci series up to n\n",
      "...     \"\"\"Print a Fibonacci series up to n.\"\"\"\n",
      "...     a, b = 0, 1\n",
      "...     while a < n:\n",
      "...         print(a, end=' ')\n",
      "...         a, b = b, a+b\n",
      "...     print()\n",
      "...\n",
      ">>> # Now call the function we just defined:\n",
      ">>> fib(2000)\n",
      "0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597\n",
      "```\n",
      "\n",
      "The keyword [`def`](../reference/compound_stmts.html#def) introduces a function *definition*. It must be\n",
      "followed by the function name and the parenthesized list of formal parameters.\n",
      "The statements that form the body of the function start at the next line, and\n",
      "must be indented.\n",
      "\n",
      "The first statement of the function body can optionally be a string literal;\n",
      "this string literal is the function’s documentation string, or *docstring*.\n",
      "(More about docstrings can be found in the section [Documentation Strings](#tut-docstrings).)\n",
      "There are tools which use docstrings to automatically produce online or printed\n",
      "documentation, or to let the user interactively browse through code; it’s good\n",
      "practice to include docstrings in code that you write, so make a habit of it.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query only original content\n",
    "retriever.search_kwargs = {\"k\": 2, \"filter\": {\"type\": \"original\"}}\n",
    "original_results = retriever.invoke(\"I want to write a Python script that prints numbers from 1 to 30.\")\n",
    "print_results(\"Original Content Results.\", original_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Original Content Results** contain two results retrieved from the original content, providing more detailed and comprehensive information compared to the summaries.\n",
    "\n",
    "**First Result**: The first result is relevant to the query, as it focuses on the `range()` function, which is directly related to printing numbers in Python. The output provides a detailed explanation of how the `range()` function can be used to iterate over a sequence of numbers and includes an example demonstrating its use, which aligns well with the query.\n",
    "\n",
    "**Second Result**: The second result, however, is less relevant to the query. It discusses creating a Fibonacci series function and includes information on _docstrings_. While useful in other contexts, this output doesn't directly relate to the task of writing a Python script to print numbers from 1 to 30, making it a less accurate match compared to the first result.\n",
    "\n",
    "When comparing this to the **Querying Summaries** example, the second result in **Querying Original Content** is not as closely aligned with the original query. The **summary** query provided concise, context-rich information about `range()` and its use for iterating over numbers, which was a better match for the specific task.\n",
    "\n",
    "### Querying Both Summaries and Original Content\n",
    "\n",
    "It is also possible to query both summaries and original content together without applying any filters, allowing retrieval of a mix of both types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Results.\n",
      "\n",
      "---------- Result #1 ----------\n",
      "Metadata: {'Header 1': '4. More Control Flow Tools', 'Header 2': '4.3. The [`range()`](../library/stdtypes.html#range \"range\") Function', 'summary_id': '1ff36254-aa51-4302-b0f1-aeba92212a96', 'type': 'original'}\n",
      "Content: If you do need to iterate over a sequence of numbers, the built\\-in function\n",
      "[`range()`](../library/stdtypes.html#range \"range\") comes in handy. It generates arithmetic progressions:\n",
      "\n",
      "```\n",
      ">>> for i in range(5):\n",
      "...     print(i)\n",
      "...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "```\n",
      "\n",
      "The given end point is never part of the generated sequence; `range(10)` generates\n",
      "10 values, the legal indices for items of a sequence of length 10\\. It\n",
      "is possible to let the range start at another number, or to specify a different\n",
      "increment (even negative; sometimes this is called the ‘step’):\n",
      "\n",
      "```\n",
      ">>> list(range(5, 10))\n",
      "[5, 6, 7, 8, 9]\n",
      "\n",
      ">>> list(range(0, 10, 3))\n",
      "[0, 3, 6, 9]\n",
      "\n",
      ">>> list(range(-10, -100, -30))\n",
      "[-10, -40, -70]\n",
      "```\n",
      "\n",
      "To iterate over the indices of a sequence, you can combine [`range()`](../library/stdtypes.html#range \"range\") and\n",
      "[`len()`](../library/functions.html#len \"len\") as follows:\n",
      "\n",
      "```\n",
      ">>> a = ['Mary', 'had', 'a', 'little', 'lamb']\n",
      ">>> for i in range(len(a)):\n",
      "...     print(i, a[i])\n",
      "...\n",
      "0 Mary\n",
      "1 had\n",
      "2 a\n",
      "3 little\n",
      "4 lamb\n",
      "```\n",
      "\n",
      "In most such cases, however, it is convenient to use the [`enumerate()`](../library/functions.html#enumerate \"enumerate\")\n",
      "function, see [Looping Techniques](datastructures.html#tut-loopidioms).\n",
      "\n",
      "\n",
      "---------- Result #2 ----------\n",
      "Metadata: {'id': 'd0f21c29-decf-405b-828f-8b714eebd459', 'type': 'summary'}\n",
      "Content: The `range()` function returns an object that behaves like a list but doesn't actually make one, saving space. This iterable object can be used with functions and constructs that expect successive items until the supply is exhausted, such as the `for` statement or the `sum()` function. When printed directly, it displays its start and end values, e.g., `range(0, 10)`.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query both summaries and original content\n",
    "retriever.search_kwargs = {\"k\": 2}  # No filter to get both types\n",
    "both_results = retriever.invoke(\"I want to write a Python script that prints numbers from 1 to 30.\")\n",
    "print_results(\"Both Results.\", both_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Both Results** query retrieves a mix of both original content and summaries, offering different levels of detail.\n",
    "\n",
    "**First Result (Original Content)**: This result provides a detailed explanation of the `range()` function from the original content, including examples and usage patterns. It offers a comprehensive overview of how to generate sequences of numbers using `range()`, directly aligning with the task of printing numbers from 1 to 30. The examples demonstrate various ways to use `range()` with different parameters, making it a very relevant and informative result.\n",
    "\n",
    "**Second Result (Summary)**: The second result is a concise summary that focuses on the functionality of the `range()` function, highlighting its efficiency and how it behaves like a list when iterated over. It provides a more compact but accurate explanation relevant to the query.\n",
    "\n",
    "Both results are highly relevant to the query, with the original content offering depth and the summary offering a concise explanation.\n",
    "\n",
    "### Querying Summaries but Retrieving Original Text\n",
    "\n",
    "One of the most powerful aspects of this process is the ability to query using summaries while retrieving the corresponding original text. This approach allows for quick, concise queries with summaries, followed by retrieval of the full original content based on those summaries.\n",
    "\n",
    "In this step, summaries are queried, and the summary IDs are used to fetch the original content. This method is particularly useful when detailed content is needed but the speed and efficiency of querying summaries is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Texts based on Summary Query.\n",
      "\n",
      "---------- Result #1 ----------\n",
      "Metadata: {'Header 1': '4. More Control Flow Tools', 'Header 2': '4.3. The [`range()`](../library/stdtypes.html#range \"range\") Function', 'summary_id': 'd0f21c29-decf-405b-828f-8b714eebd459', 'type': 'original'}\n",
      "Content: A strange thing happens if you just print a range:\n",
      "\n",
      "```\n",
      ">>> range(10)\n",
      "range(0, 10)\n",
      "```\n",
      "\n",
      "In many ways the object returned by [`range()`](../library/stdtypes.html#range \"range\") behaves as if it is a list,\n",
      "but in fact it isn’t. It is an object which returns the successive items of\n",
      "the desired sequence when you iterate over it, but it doesn’t really make\n",
      "the list, thus saving space.\n",
      "\n",
      "We say such an object is [iterable](../glossary.html#term-iterable), that is, suitable as a target for\n",
      "functions and constructs that expect something from which they can\n",
      "obtain successive items until the supply is exhausted. We have seen that\n",
      "the [`for`](../reference/compound_stmts.html#for) statement is such a construct, while an example of a function\n",
      "that takes an iterable is [`sum()`](../library/functions.html#sum \"sum\"):\n",
      "\n",
      "```\n",
      ">>> sum(range(4))  # 0 + 1 + 2 + 3\n",
      "6\n",
      "```\n",
      "\n",
      "Later we will see more functions that return iterables and take iterables as\n",
      "arguments. In chapter [Data Structures](datastructures.html#tut-structures), we will discuss in more detail about\n",
      "[`list()`](../library/stdtypes.html#list \"list\").\n",
      "\n",
      "\n",
      "---------- Result #2 ----------\n",
      "Metadata: {'Header 1': '4. More Control Flow Tools', 'Header 2': '4.3. The [`range()`](../library/stdtypes.html#range \"range\") Function', 'summary_id': '1ff36254-aa51-4302-b0f1-aeba92212a96', 'type': 'original'}\n",
      "Content: If you do need to iterate over a sequence of numbers, the built\\-in function\n",
      "[`range()`](../library/stdtypes.html#range \"range\") comes in handy. It generates arithmetic progressions:\n",
      "\n",
      "```\n",
      ">>> for i in range(5):\n",
      "...     print(i)\n",
      "...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "```\n",
      "\n",
      "The given end point is never part of the generated sequence; `range(10)` generates\n",
      "10 values, the legal indices for items of a sequence of length 10\\. It\n",
      "is possible to let the range start at another number, or to specify a different\n",
      "increment (even negative; sometimes this is called the ‘step’):\n",
      "\n",
      "```\n",
      ">>> list(range(5, 10))\n",
      "[5, 6, 7, 8, 9]\n",
      "\n",
      ">>> list(range(0, 10, 3))\n",
      "[0, 3, 6, 9]\n",
      "\n",
      ">>> list(range(-10, -100, -30))\n",
      "[-10, -40, -70]\n",
      "```\n",
      "\n",
      "To iterate over the indices of a sequence, you can combine [`range()`](../library/stdtypes.html#range \"range\") and\n",
      "[`len()`](../library/functions.html#len \"len\") as follows:\n",
      "\n",
      "```\n",
      ">>> a = ['Mary', 'had', 'a', 'little', 'lamb']\n",
      ">>> for i in range(len(a)):\n",
      "...     print(i, a[i])\n",
      "...\n",
      "0 Mary\n",
      "1 had\n",
      "2 a\n",
      "3 little\n",
      "4 lamb\n",
      "```\n",
      "\n",
      "In most such cases, however, it is convenient to use the [`enumerate()`](../library/functions.html#enumerate \"enumerate\")\n",
      "function, see [Looping Techniques](datastructures.html#tut-loopidioms).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query summary but get original text\n",
    "retriever.search_kwargs = {\"k\": 2, \"filter\": {\"type\": \"summary\"}}\n",
    "summary_for_original_results = retriever.invoke(\"I want to write a Python script that prints numbers from 1 to 30.\")\n",
    "\n",
    "# Extract original texts based on summary query\n",
    "original_texts_based_on_summary = []\n",
    "if summary_for_original_results:\n",
    "    summary_ids = [result.metadata[\"id\"] for result in summary_for_original_results]\n",
    "    for summary_id in summary_ids:\n",
    "        retriever.search_kwargs = {\"k\": 2, \"filter\": {\"summary_id\": summary_id}}\n",
    "        original_texts_based_on_summary.extend(\n",
    "            retriever.invoke(\"\")\n",
    "        )\n",
    "\n",
    "print_results(\"Original Texts based on Summary Query.\", original_texts_based_on_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Original Texts based on Summary Query** feature demonstrates the power of querying using summaries while retrieving the corresponding original content. In this case, the summaries point to sections of the document that discuss Python's `range()` function, and the retrieved original content provides detailed explanations, examples, and usage.\n",
    "\n",
    "**First Result**: This result explains how the `range()` function behaves like a list, demonstrating its use in conjunction with the `for` loop and the `sum()` function. The example shows how `range()` returns an iterable object that saves memory, directly relating to the query about printing numbers.\n",
    "\n",
    "**Second Result**: This result provides additional examples of how the `range()` function can be used to iterate over numbers, including various ways to specify a range, starting point, step size, and more. This content is a good match for the query, as it focuses on iterating over sequences of numbers in Python.\n",
    "\n",
    "Compared to querying summaries alone, this method offers a quick way to retrieve concise, context-rich summaries and use them to fetch detailed original content. Both retrieved results are highly relevant to the query about writing a Python script to print numbers from 1 to 30, offering detailed explanations and code examples on how to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook outlines the process of preparing technical documentation for use in Retrieval-Augmented Generation (RAG) applications. It begins by loading and converting documents into Markdown format, preserving both structure and code blocks. The document is then split into manageable chunks based on headers and token counts to maintain context and enable efficient querying.\n",
    "\n",
    "Summarization plays a crucial role in this process, with summaries generated from both _chunk content_ and _hierarchical headers_ to preserve full context. These summaries, along with the original content, are stored in a VectorDB (Chroma), supporting flexible querying.\n",
    "\n",
    "The final and perhaps most powerful aspect is the ability to query summaries while retrieving the original text. Summaries are often more efficient, as they incorporate not only content but also relevant headers to provide additional context. This method facilitates fast, concise queries while ensuring access to detailed, in-depth content when needed.\n",
    "\n",
    "Each step in this workflow helps preserve the logical flow, integrity, and usability of the content. Summarization optimizes performance, while flexible querying enables efficient information retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
